{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marktfaust/Linear-Classifiers-Feedforward-Neural-Networks/blob/main/Mark_Faust_DL_Homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework \\#2: Linear Classifiers and Feedforward Neural Networks  (37 Total Points)"
      ],
      "metadata": {
        "id": "OYIWWqT-cs-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework assignment, we will (1) implement a linear model for classification (i.e. categorical regression), (2) implement evaluation metrics like accuracy, prediction, recall, and F1 score, implement a one-hidden-layer neural network, using both (3) manual backpropagation and (4) autodiff supported backpropagation.  Lastly, we will use a held-out validation set to choose the appropriate number of hidden units (5) and layers (6).\n",
        "\n",
        "Let's start by importing the usual libraries: NumPy and Matplotlib.  We will also import [SciKit Learn](https://https://scikit-learn.org/stable/index.html), as it allows us to load the synthetic data set on which we will train our models."
      ],
      "metadata": {
        "id": "X0HOEHi4c371"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons"
      ],
      "metadata": {
        "id": "WBHcBf6Acosv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Visualizing the Data\n",
        "Let's first load and visualize the data on which we will be training the models.  We will have a small dataset of only 15 points."
      ],
      "metadata": {
        "id": "vJF7-fAMd_r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset with three crescent-moon-shaped clusters\n",
        "def make_three_moons(n_samples=300, noise=0.15, random_state=42):\n",
        "    X, y = make_moons(n_samples=n_samples // 3 * 2, noise=noise, random_state=random_state)\n",
        "\n",
        "    # Extract one class and rotate it to create a third class\n",
        "    X_class_1 = X[y == 1]\n",
        "    theta = np.pi\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    X_rotated = X_class_1 @ rotation_matrix\n",
        "\n",
        "    # Shift the new class\n",
        "    X_rotated = X_rotated + np.array([3.25, .5])  # Shift up and to the right\n",
        "\n",
        "    # Create new labels\n",
        "    y_rotated = np.full(len(X_rotated), 2)\n",
        "\n",
        "    # Combine original and new data\n",
        "    X_new = np.vstack([X, X_rotated])\n",
        "    y_new = np.hstack([y, y_rotated])\n",
        "\n",
        "    # shuffle so that the 3rd class isn't always at the end\n",
        "    index = np.arange(X_new.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    return X_new[index], y_new[index]\n",
        "\n",
        "# Generate the training set\n",
        "X, y = make_three_moons(n_samples=15)\n",
        "n_classes = 3\n",
        "\n",
        "# Visualize the features and color-code by the class assignments\n",
        "colors = {0: '#005AB5', 1: '#E1BE6A', 2: '#DC3220'}\n",
        "plt.scatter(X[:, 0], X[:, 1], c=[colors[y_idx] for y_idx in y], s=100, edgecolor='k')\n",
        "plt.title(\"Feature Space, Color-Coded by Classes (3)\", fontsize=15)\n",
        "plt.xlabel(\"$X_1$\", fontsize=20)\n",
        "plt.ylabel(\"$X_2$\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a-8Mj7smeLJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Implementing a Linear Classifier (3 points)\n",
        "Now let's implement a classifier for this data and start with a relatively simple model, although we will re-use much of our code for building neural networks.\n",
        "\n",
        "Let's import JAX to get started, but note for now, we will only be using its autodiff capabilities to test our manual solutions."
      ],
      "metadata": {
        "id": "_TaT9oI8hpDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad"
      ],
      "metadata": {
        "id": "z5SI0GeGiPED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will define two crucial functions: the softmax inverse link function and the categorical cross-entropy function."
      ],
      "metadata": {
        "id": "yG0-Dm_yijXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    exp_z = jnp.exp(z)\n",
        "    return exp_z / jnp.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cat_cross_entropy_loss(Y, class_probs):\n",
        "    return jnp.mean(jnp.sum(-Y * jnp.log(class_probs + 1e-8), axis=1))"
      ],
      "metadata": {
        "id": "fkq0ozbLitPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to define the crucial function that computes the gradient with which we can implement gradient descent.  For the model $$ \\mathbb{E}[\\mathbf{y} | \\mathbf{x}] \\ = \\ \\boldsymbol{\\pi} \\ = \\  \\text{softmax}(\\mathbf{W}^{T} \\mathbf{x}),$$ where $\\boldsymbol{\\pi}$ is the categorical distribution's parameters and $W \\in \\mathbb{R}^{D \\times K}$ is a matrix of parameters (weights), compute the gradient of the categorical cross-entropy function w.r.t. $\\mathbf{W}$: $$\\nabla_{\\mathbf{W}} \\ell(\\mathbf{W}; D),$$ where $\\ell$ is the categorical cross entropy function and $D$ is the training data.  See [Section 2.3 of the course notes](https://https://enalisnick.github.io/Deep_Learning_Course_Notes.pdf#page=19.47) if you need reminded of the gradient derivation / equations.  \n",
        "\n",
        "HINT: While I have written in the model above with the features as being a vector (like in class and in the notes), given an $N \\times D$ feature *matrix*, it's computationally more efficient to implement the model as: $$ \\mathbb{E}[\\mathbf{Y} | \\mathbf{X}] \\ = \\ \\boldsymbol{\\Pi} \\ = \\  \\text{softmax}(\\mathbf{X}\\mathbf{W}),$$ where $\\boldsymbol{\\Pi}$ is an $N \\times K$ matrix of class probabilities, $\\mathbf{X}$ is a $N \\times D$ matrix of features, and $\\mathbf{W}\\in \\mathbb{R}^{D \\times K}$ is the same as above."
      ],
      "metadata": {
        "id": "P_cAdGDsjvfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_linear_model(X, Y, weights):\n",
        "  ### HW POINTS: 3\n",
        "  ### Input ###\n",
        "  # X: N x D matrix of features\n",
        "  # Y: N x K matrix of one-hot encoded labels\n",
        "  # weights: D x K matrix of model parameters\n",
        "  ### Output ###\n",
        "  # return a (DxK)-sized matrix of gradients\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # compute class probabilities\n",
        "  # compute difference between probs and labels\n",
        "  # combine with feature vectors\n",
        "  # return ???"
      ],
      "metadata": {
        "id": "YdTR_X6Qj588"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, let's check if the implementation is correct by comparing it to the autodiff result."
      ],
      "metadata": {
        "id": "XcYFkwVpZwqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define training loss for autograd\n",
        "def train_loss_linear_model(weights, X, Y):\n",
        "  class_probs = softmax(jnp.dot(X, weights))\n",
        "  return cat_cross_entropy_loss(Y, class_probs)\n",
        "\n",
        "# sample some random weights\n",
        "weights = np.random.normal(size=(X.shape[1], n_classes))\n",
        "\n",
        "# convert y labels to one-hot-encoded matrix\n",
        "Y_one_hot = jnp.eye(n_classes)[y]\n",
        "\n",
        "# compute gradients\n",
        "manual_grad_w = compute_gradient_linear_model(X, Y_one_hot, weights)\n",
        "auto_grad_w = grad(train_loss_linear_model)(weights, X, Y_one_hot)\n",
        "\n",
        "# check if the gradient vectors are close; if so, test passes\n",
        "print(\"Are the gradient vectors equal (approximately, up to 1e-3)?\")\n",
        "print(jnp.allclose(auto_grad_w, manual_grad_w, rtol=.001))"
      ],
      "metadata": {
        "id": "MY20tboKZ57T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your test passes (i.e. 'True' is printed above), lets train the model and visualize the results."
      ],
      "metadata": {
        "id": "gkyd4O33kuuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAINING HYPERPARAMETERS FOR THE USER TO CONFIGURE ###\n",
        "# stepsize for gradient descent\n",
        "stepsize = 0.5\n",
        "\n",
        "# maximum number of times to iterate the gradient update\n",
        "max_iterations = 250\n",
        "#################################################\n",
        "\n",
        "# add ones to feature matrix to represent offset parameter\n",
        "X_w_ones = jnp.c_[X, np.ones(X.shape[0])]\n",
        "\n",
        "# initialize weights, scaling them by the number of input feature dimensions\n",
        "weights = np.random.normal(size=(X_w_ones.shape[1], n_classes)) * jnp.sqrt(2./X_w_ones.shape[1])\n",
        "\n",
        "# run gradient descent\n",
        "losses = []\n",
        "for i in range(max_iterations):\n",
        "  gradient = compute_gradient_linear_model(X_w_ones, Y_one_hot, weights)\n",
        "  weights -= stepsize * gradient\n",
        "  losses.append(train_loss_linear_model(weights, X_w_ones, Y_one_hot))\n",
        "\n",
        "# plot the training curve\n",
        "plt.plot(range(1, max_iterations+1), losses, lw=5)\n",
        "plt.xlabel(\"Gradient Descent Iteration\", fontsize=12)\n",
        "plt.ylabel(\"Categorical Cross-Entropy Loss\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ifcEAMIlkypm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Decision Boundaries\n",
        "def plot_decision_boundary(weights, X, y):\n",
        "    # make feature grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 300), jnp.linspace(y_min, y_max, 300))\n",
        "\n",
        "    # compute predictions\n",
        "    Z = softmax(jnp.dot(jnp.c_[xx.ravel(), yy.ravel(), jnp.ones(300*300)], weights))\n",
        "    Z = jnp.argmax(Z, axis=1).reshape(xx.shape)\n",
        "\n",
        "    # generate colors for data and classifier prediction regions\n",
        "    plt.contourf(xx, yy, Z, levels=4, alpha=0.5, colors=[colors[0], colors[2], colors[1]])\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=[colors[y_idx] for y_idx in y], s=100, edgecolor='k')\n",
        "    plt.xlabel(\"$X_1$\", fontsize=20)\n",
        "    plt.ylabel(\"$X_2$\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_decision_boundary(weights, X, y)"
      ],
      "metadata": {
        "id": "q82pBf7Cdx9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting figure should look roughly like Figure 5 of the notes but with fewer data points."
      ],
      "metadata": {
        "id": "zNYTuhlyEV7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Evaluating the Classifier (5 points)\n",
        "Now that we have fit a classifier, let's examine just how good a model it is.  Below we will collect 500 additional data points that the model has never seen before.  This can be our test set (or validation set, if we choose to revise the model hyperparameters) to assess the performance of the model on held-out data."
      ],
      "metadata": {
        "id": "fC0_iwAiJgZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new points as a validation / test set\n",
        "X_valid, y_valid = make_three_moons(n_samples=500)\n",
        "Y_one_hot_valid = jnp.eye(n_classes)[y_valid]\n",
        "X_w_ones_valid = jnp.c_[X_valid, np.ones(X_valid.shape[0])]\n",
        "\n",
        "\n",
        "# Visualize the features and color-code by the class assignments\n",
        "plt.scatter(X_valid[:, 0], X_valid[:, 1], c=[colors[y_idx] for y_idx in y_valid], s=100, edgecolor='k')\n",
        "plt.title(\"Feature Space, Color-Coded by Classes (3)\", fontsize=15)\n",
        "plt.xlabel(\"$X_1$\", fontsize=20)\n",
        "plt.ylabel(\"$X_2$\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_-_YuEwJ2if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first implement *accuracy*; that is, the number of times the classifier makes the correct prediction out of the total number of predictions it makes (500, in this case, one for every validation point)."
      ],
      "metadata": {
        "id": "DEkRFg3XONjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y, predictions):\n",
        "  #### HW POINTS: 1\n",
        "  ### Input ###\n",
        "  # y: M-sized array of true labels (integers)\n",
        "  # predictions: M-sized array of predicted labels (integers)\n",
        "  ### Output ###\n",
        "  # real-valued scalar corresponding to accuracy\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # return ???\n",
        "\n",
        "#compute predictions on train set\n",
        "train_class_probs = softmax(jnp.dot(X_w_ones, weights))\n",
        "train_predictions = jnp.argmax(train_class_probs, axis=1)\n",
        "\n",
        "# compute prediction on new data\n",
        "valid_class_probs = softmax(jnp.dot(X_w_ones_valid, weights))\n",
        "valid_predictions = jnp.argmax(valid_class_probs, axis=1)\n",
        "\n",
        "# print results\n",
        "print(\"Your implementation...\")\n",
        "print(\"Training Accuracy: %.1f %%\" %(compute_accuracy(y, train_predictions)*100))\n",
        "print(\"Validation Accuracy: %.1f %%\" %(compute_accuracy(y_valid, valid_predictions)*100))\n",
        "\n",
        "# check against SciKit Learn's built-in implementation\n",
        "from sklearn.metrics import accuracy_score as skl_accuracy\n",
        "print(\"\\nSciKit Learn's implementation...\")\n",
        "print(\"Training Accuracy: %.1f %%\" %(skl_accuracy(y, train_predictions)*100))\n",
        "print(\"Validation Accuracy: %.1f %%\" %(skl_accuracy(y_valid, valid_predictions)*100))"
      ],
      "metadata": {
        "id": "YrdUURxjKOPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your implementation matches SciKit Learn's, you can now move on to the next step: computing the confusion matrix.  Let's use Scikit Learn's built-in function to compute the confusion matrix on the validation data..."
      ],
      "metadata": {
        "id": "RhZOrPVQMXkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c_mat_valid = confusion_matrix(y_valid, valid_predictions)\n",
        "print(c_mat_valid)"
      ],
      "metadata": {
        "id": "YgyB6TpqLbW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this confusion matrix, implement functions to compute precision, recall, and the F1 score.  Recall that precision is the number of times the classifier makes the correct prediction out of the total times is makes that prediction.  Then average that values over all classes.  Recall is the number of times the classifier makes the correct prediction out of the total number of instances of that class.  Again, average over all classes.  Lastly, (macro) F1 score is the harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "zMWWrcOrPCNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_precision(confusion_mat):\n",
        "  #### HW POINTS: 1\n",
        "  ### Input ###\n",
        "  # confusion_mat: KxK confusion matrix (with truth on the rows, preds on columns)\n",
        "  ### Output ###\n",
        "  # real-valued scalar corresponding to precision\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # return ???\n",
        "\n",
        "\n",
        "def compute_recall(confusion_mat):\n",
        "  #### HW POINTS: 1\n",
        "  ### Input ###\n",
        "  # confusion_mat: KxK confusion matrix (with truth on the rows, preds on columns)\n",
        "  ### Output ###\n",
        "  # real-valued scalar corresponding to recall\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # return ???\n",
        "\n",
        "\n",
        "def compute_f1(confusion_mat):\n",
        "  #### HW POINTS: 2\n",
        "  ### Input ###\n",
        "  # confusion_mat: KxK confusion matrix (with truth on the rows, preds on columns)\n",
        "  ### Output ###\n",
        "  # real-valued scalar corresponding to F1 score\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # return ???\n",
        "\n",
        "# print results\n",
        "print(\"Your implementation...\")\n",
        "print(\"Validation Precision: %.3f\" %(compute_precision(c_mat_valid)))\n",
        "print(\"Validation Recall: %.3f\" %(compute_recall(c_mat_valid)))\n",
        "print(\"Validation F1 Score: %.3f\" %(compute_f1(c_mat_valid)))\n",
        "\n",
        "# check against SciKit Learn's built-in implementation\n",
        "from sklearn.metrics import precision_score as skl_precision\n",
        "from sklearn.metrics import recall_score as skl_recall\n",
        "from sklearn.metrics import f1_score as skl_f1\n",
        "print(\"\\nSciKit Learn's implementation...\")\n",
        "print(\"Validation Precision: %.3f\" %(skl_precision(y_valid, valid_predictions, average='macro')))\n",
        "print(\"Validation Recall: %.3f\" %(skl_recall(y_valid, valid_predictions, average='macro')))\n",
        "print(\"Validation F1 Score: %.3f\" %(skl_f1(y_valid, valid_predictions, average='macro')))"
      ],
      "metadata": {
        "id": "Ec0BtkbkPBl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Admittedly, these precision, recall, and F1 scores are not so interesting in this case since the number of points assigned to each class is well-balanced.  These become increasingly important as classes become increasingly imbalanced."
      ],
      "metadata": {
        "id": "Xb4NcxaXyL-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Implementing a One-Hidden-Layer Neural Network (12 points)\n",
        "\n",
        "Now we move on to define a more powerful model: a one-hidden layer neural network.  Let's start by implementing the non-linear activation function for the hidden units, the logistic function."
      ],
      "metadata": {
        "id": "kLIItRXSqxmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-linear activation function for hidden layer\n",
        "def logistic_activation(z):\n",
        "  return 1 / (1 + jnp.exp(-z))"
      ],
      "metadata": {
        "id": "scB1pzhzsnSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement the forward propagation of the neural network.  The hidden units will be computed as:\n",
        "$$\\mathbf{h} \\ = \\ \\text{logistic}( \\mathbf{W}_{0}^{T} \\mathbf{x}) $$ where $x$ is a D-dimensional feature vector, $W_{0} \\in \\mathbb{R}^{D \\times H}$ is weights that connect the input layer to the hidden layer, and logistic$(\\cdot)$ is the logistic function applied element-wise to all $H$ hidden dimensions.  After computing the hidden units, the class probabilities will be compute simiarly to above: $$ \\mathbb{E}[\\mathbf{y} | \\mathbf{x}] \\ = \\ \\boldsymbol{\\pi} \\ = \\  \\text{softmax}(\\mathbf{W}_{1}^{T} \\mathbf{h}),$$ where $\\mathbf{W}_{1} \\in \\mathbb{R}^{(H+1) \\times K}$ is the matrix of parameters that connects the hidden layer to the output layer.  The first dimension is of size $H+1$ to account for the addition offset dimension that is added to the hidden layer.\n",
        "\n",
        "HINT: Again, you'll likely find it to be more efficient to implement the weight multiplications as $\\mathbf{H} \\ = \\ \\text{logistic}(  \\mathbf{X}\\mathbf{W}_{0}) $ where $\\mathbf{H} \\in (0,1)^{N \\times H}$ and $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$.  The hidden-to-output layer would follow the same format as for categorical linear regression, except $\\mathbf{H}$ is now the 'feature' matrix.  Be sure to add a column on 1's to it to represent the offset parameter."
      ],
      "metadata": {
        "id": "jJdcPJEQ3GM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop_simple_nn(X, weights):\n",
        "  ### HW POINTS: 6\n",
        "  ### Input ###\n",
        "  # X: N x D matrix of features\n",
        "  # weights: list([D x H matrix, (H+1) x K matrix])\n",
        "  ### Output ###\n",
        "  # return: list([N x K matrix of class probs, N x (H+1) matrix of hidden units])\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # compute hidden layer pre-activation\n",
        "  # apply activation function\n",
        "  # append column of ones for offset\n",
        "  # compute output pre-activation\n",
        "  # apply softmax transformation\n",
        "  # return both the class probabilities *AND* the hidden represenation\n",
        "  # return ???"
      ],
      "metadata": {
        "id": "qJsi_36NtJ7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here will will implement the gradient calculation for both $\\mathbf{W}_{0}$ and $\\mathbf{W}_{1}$.  Your code should return a list with two elements, as shown below:\n",
        "$$ \\left[ \\left(\\nabla_{\\mathbf{W}_{0}} \\ell(\\mathbf{W}_{0}, \\mathbf{W}_{1}; D)\\right)^{T}, \\ \\  \\left(\\nabla_{\\mathbf{W}_{1}} \\ell(\\mathbf{W}_{0}, \\mathbf{W}_{1}; D)\\right)^{T} \\right], $$ where $\\ell$ is the categorical cross-entropy loss function.  That is, the first element should be the gradient for the input-to-hidden weights (re-oriented to match the orientation of $\\mathbf{W}_{0}$), and the second element should be the gradient for the hidden-to-output weights."
      ],
      "metadata": {
        "id": "bdCE2Xt25l5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_simple_nn(X, Y, weights):\n",
        "  ### HW POINTS: 6\n",
        "  ### Input ###\n",
        "  # X: N x D matrix of features\n",
        "  # Y: N x K matrix of one-hot labels\n",
        "  # weights: list([D x H matrix, (H+1) x K matrix])\n",
        "  ### Output ###\n",
        "  # return: list([D x H gradient matrix, (H+1) x K gradient matrix])\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # compute the forward propagation\n",
        "  # compute d loss / d W1 (see linear model implementation)\n",
        "  # compute d loss / d h\n",
        "  # compute d h / d pre-activation\n",
        "  # compute d pre-activation / d W0\n",
        "  # return ???"
      ],
      "metadata": {
        "id": "-QCk7wNesQvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the implementation against the gradients computed by autograd."
      ],
      "metadata": {
        "id": "iHrUPnm-CUzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define training loss for autograd\n",
        "def train_loss_simple_nn(weights, X, Y):\n",
        "  class_probs = forward_prop_simple_nn(X, weights)[0]\n",
        "  return cat_cross_entropy_loss(Y, class_probs)\n",
        "\n",
        "# sample some random weights\n",
        "n_hidden_units = 25\n",
        "weights = [np.random.normal(size=(X.shape[1], n_hidden_units)),\n",
        "           np.random.normal(size=(n_hidden_units + 1, n_classes))]\n",
        "\n",
        "\n",
        "# compute gradients\n",
        "manual_grad_w = compute_gradient_simple_nn(X, Y_one_hot, weights)\n",
        "auto_grad_w = grad(train_loss_simple_nn)(weights, X, Y_one_hot)\n",
        "\n",
        "# check if the gradient vectors are close; if so, test passes\n",
        "print(\"Are the gradient vectors equal for the input-to-hidden weights (approximately, up to 1e-3)?\")\n",
        "print(jnp.allclose(auto_grad_w[0], manual_grad_w[0], rtol=.001))\n",
        "\n",
        "print(\"Are the gradient vectors equal for the hidden-to-output weights (approximately, up to 1e-3)?\")\n",
        "print(jnp.allclose(auto_grad_w[1], manual_grad_w[1], rtol=.001))"
      ],
      "metadata": {
        "id": "WXHA902QyUGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAINING HYPERPARAMETERS FOR THE USER TO CONFIGURE ###\n",
        "# number of hidden units in the neural network\n",
        "n_hidden_units = 10\n",
        "\n",
        "# stepsize for gradient descent\n",
        "stepsize = 0.5\n",
        "\n",
        "# maximum number of times to iterate the gradient update\n",
        "max_iterations = 2500\n",
        "#################################################\n",
        "\n",
        "# initialize weights\n",
        "weights = [np.random.normal(size=(X_w_ones.shape[1], n_hidden_units)) * jnp.sqrt(2./X_w_ones.shape[1]),\n",
        "           np.random.normal(size=(n_hidden_units + 1, n_classes)) * jnp.sqrt(2./(n_hidden_units+1))]\n",
        "\n",
        "# run gradient descent\n",
        "losses = []\n",
        "for i in range(max_iterations):\n",
        "  gradient = compute_gradient_simple_nn(X_w_ones, Y_one_hot, weights)\n",
        "  weights[0] -= stepsize * gradient[0]\n",
        "  weights[1] -= stepsize * gradient[1]\n",
        "  losses.append(train_loss_simple_nn(weights, X_w_ones, Y_one_hot))\n",
        "\n",
        "# plot the training curve\n",
        "plt.plot(range(1, max_iterations+1), losses, lw=5)\n",
        "plt.xlabel(\"Gradient Descent Iteration\", fontsize=12)\n",
        "plt.ylabel(\"Categorical Cross-Entropy Loss\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TltTAX1j0PG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Decision Boundaries\n",
        "def plot_nn_decision_boundary(weights, X, y, forward_prop=forward_prop_simple_nn):\n",
        "    # make feature grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 300), jnp.linspace(y_min, y_max, 300))\n",
        "\n",
        "    # compute predictions\n",
        "    Z = forward_prop_simple_nn(jnp.c_[xx.ravel(), yy.ravel(), jnp.ones(300*300)], weights)[0]\n",
        "    Z = jnp.argmax(Z, axis=1).reshape(xx.shape)\n",
        "\n",
        "    # generate colors\n",
        "    plt.contourf(xx, yy, Z, levels=4, alpha=0.5, colors=[colors[0], colors[2], colors[1]])\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=[colors[y_idx] for y_idx in y], s=100, edgecolor='k')\n",
        "    plt.xlabel(\"$X_1$\", fontsize=20)\n",
        "    plt.ylabel(\"$X_2$\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_nn_decision_boundary(weights, X, y)"
      ],
      "metadata": {
        "id": "hxwLSPUX1b4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Implementing a Neural Network with an Arbitrary Depth (7 Points)\n",
        "Now let's move on to a more powerful model still: a neural network with an arbitrary depth.  Fortunately, here we will use autograd to do the 'heavy lifting' of performing the gradient calculations.  Thus, we only need to implement the forward propagation of the model.  Specifically, it should implement for $l \\in [1, L]$ hidden layers:\n",
        "$$\\mathbf{h}_{l} \\ = \\ \\text{logistic}( \\mathbf{W}_{l-1}^{T} \\mathbf{h}_{l-1}) $$ where $\\mathbf{h}_{0} = \\mathbf{x}$, meaning that the first 'hidden layer' is just the original feature vector.  $\\mathbf{h}_{L}$ should then be transformed one more time, with a $(H+1) \\times K$ matrix, to get the input to the softmax transformation.  HINT: Do NOT hardcode the number of hidden layer.  In fact, you don't even need to pass the number of layers in as an argument.  Rather, let the data structure with which you store the weights implicitly encode the number of layers."
      ],
      "metadata": {
        "id": "hbgrvSsEmuDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop_deep_nn(X, weights):\n",
        "  ### HW POINTS: 5\n",
        "  ### Input ###\n",
        "  # X: N x D matrix of features\n",
        "  # weights: list containing the NN's weight matrices in IN x OUT orientation\n",
        "  ### Output ###\n",
        "  # return: N x K matrix of class probabilities\n",
        "  ##############\n",
        "  # YOUR SOLUTION GOES HERE #\n",
        "  # Loop:\n",
        "  #   compute hidden layer pre-activation\n",
        "  #   apply activation function\n",
        "  #   append column of ones for offset\n",
        "  # compute output pre-activation\n",
        "  # apply softmax transformation\n",
        "  # return ???"
      ],
      "metadata": {
        "id": "cWa_R2gcnI5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check you solution via autograd again.  I've already defined the loss function for you below, which is just computing the class probabilities with you forward_prop function and putting them into the cross-entropy loss from above."
      ],
      "metadata": {
        "id": "zv_GYctK3_6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define training loss for autograd\n",
        "def train_loss_deep_nn(weights, X, Y):\n",
        "  class_probs = forward_prop_deep_nn(X, weights)\n",
        "  return cat_cross_entropy_loss(Y, class_probs)"
      ],
      "metadata": {
        "id": "Z8N4Mb5grRc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to implement the call to autograd to get the function that computes the gradients for an arbitrary number of weight matrices."
      ],
      "metadata": {
        "id": "a8oWq4Kc4LuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Use JAX to compute a function for the gradient for all weight parameters\n",
        "### HW POINTS: 2\n",
        "# compute_auto_gradient_deep_nn = ???"
      ],
      "metadata": {
        "id": "6_ea2kzXscxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's train the model with the code below.  Feel free to vary the hyperparameters, taking note of how the number of layers, hidden units, and gradient descent parameters affects the training time and outcome."
      ],
      "metadata": {
        "id": "uEtf_TJo4Xyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAINING HYPERPARAMETERS FOR THE USER TO CONFIGURE ###\n",
        "# number of hidden units in the neural network\n",
        "n_hidden_units = 10\n",
        "\n",
        "# number of hidden layers in the neural network\n",
        "n_hidden_layers = 1\n",
        "\n",
        "# stepsize for gradient descent\n",
        "stepsize = .5\n",
        "\n",
        "# maximum number of times to iterate the gradient update\n",
        "max_iterations = 2500\n",
        "#################################################\n",
        "\n",
        "### initialize weights\n",
        "# input-to-hidden layer\n",
        "weights = [ np.random.normal(size=(X_w_ones.shape[1], n_hidden_units)) * jnp.sqrt(1/X_w_ones.shape[1]) ]\n",
        "\n",
        "# hidden-to-hidden layers\n",
        "for n in range(n_hidden_layers-1):\n",
        "  weights.append( np.random.normal(size=(n_hidden_units + 1, n_hidden_units)) * jnp.sqrt(1/(n_hidden_units + 1)) )\n",
        "\n",
        "# hidden-to-output layer\n",
        "weights.append( np.random.normal(size=(n_hidden_units + 1, n_classes)) * jnp.sqrt(1/(n_hidden_units + 1)) )\n",
        "\n",
        "# run gradient descent\n",
        "losses = []\n",
        "for i in range(max_iterations):\n",
        "  gradient = compute_auto_gradient_deep_nn(weights, X_w_ones, Y_one_hot)\n",
        "  for n in range(len(weights)):\n",
        "    weights[n] -= stepsize * gradient[n]\n",
        "  losses.append(train_loss_deep_nn(weights, X_w_ones, Y_one_hot))\n",
        "\n",
        "# plot the training curve\n",
        "plt.plot(range(1, max_iterations+1), losses, lw=5)\n",
        "plt.xlabel(\"Gradient Descent Iteration\", fontsize=12)\n",
        "plt.ylabel(\"Categorical Cross-Entropy Loss\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JzY1Gy6brae6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_nn_decision_boundary(weights, X, y, forward_prop=forward_prop_deep_nn)"
      ],
      "metadata": {
        "id": "g1ndsmiIuXTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) Use Validation Set to Select the Number of Hidden Units (2 Points)\n",
        "Now that we have a modular implementation that can support an arbitrary depth, we can address the question that always arises with neural networks: how should I set the architecture size?---meaning, the number of hidden units and hidden layers.  Let's first focus on the number of hidden units.  That is, the width of the hidden layer $\\mathbf{h}$.  We will do this by cross-validating on a held-out validation set that was not used for training, checking the validation loss for each hidden unit configuration."
      ],
      "metadata": {
        "id": "MABky4Jd2HCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "### TRAINING HYPERPARAMETERS FOR THE USER TO CONFIGURE ###\n",
        "# number of hidden units in the neural network\n",
        "n_hidden_list = [1, 5, 10, 15]\n",
        "\n",
        "# number of hidden layers in the neural network\n",
        "n_hidden_layers = 1\n",
        "\n",
        "# stepsize for gradient descent\n",
        "stepsize = 0.5\n",
        "\n",
        "# maximum number of times to iterate the gradient update\n",
        "max_iterations = 2500\n",
        "#################################################\n",
        "\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "best_validation_loss = np.inf\n",
        "best_weights = None\n",
        "\n",
        "for n_hidden in n_hidden_list:\n",
        "\n",
        "  ### initialize weights\n",
        "  # input-to-hidden layer\n",
        "  weights = [ np.random.normal(size=(X_w_ones.shape[1], n_hidden)) * jnp.sqrt(2./X_w_ones.shape[1]) ]\n",
        "\n",
        "  # hidden-to-hidden layers\n",
        "  for n in range(n_hidden_layers-1):\n",
        "    weights.append( np.random.normal(size=(n_hidden + 1, n_hidden)) * jnp.sqrt(2./(n_hidden + 1)) )\n",
        "\n",
        "  # hidden-to-output layer\n",
        "  weights.append( np.random.normal(size=(n_hidden + 1, n_classes)) * jnp.sqrt(2./(n_hidden + 1)) )\n",
        "\n",
        "  # run gradient descent\n",
        "  for i in range(max_iterations):\n",
        "    gradient = compute_auto_gradient_deep_nn(weights, X_w_ones, Y_one_hot)\n",
        "    for n in range(len(weights)):\n",
        "      weights[n] -= stepsize * gradient[n]\n",
        "\n",
        "  # track loss on training and validation sets\n",
        "  training_losses.append(train_loss_deep_nn(weights, X_w_ones, Y_one_hot))\n",
        "  validation_losses.append(train_loss_deep_nn(weights, X_w_ones_valid, Y_one_hot_valid))\n",
        "\n",
        "  # if loss is the best we've seen so far, save that value and the corresponding parameters\n",
        "  if validation_losses[-1] < best_validation_loss:\n",
        "    best_validation_loss = validation_losses[-1]\n",
        "    best_weights = copy.deepcopy(weights)"
      ],
      "metadata": {
        "id": "Et-wKimsKCuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at both the training and validation loss curves for each choice of hidden units."
      ],
      "metadata": {
        "id": "hPVWrVbx86uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training and validation loss for each number of hidden units\n",
        "plt.plot(n_hidden_list, validation_losses, 'rx-', lw=3, ms=10, label=\"Validation Loss\")\n",
        "plt.plot(n_hidden_list, training_losses, 'ko--', lw=3, ms=10, label=\"Training Loss\")\n",
        "plt.xlabel(\"Number of Hidden Units\", fontsize=12)\n",
        "plt.ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OXkReon_MUtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the decision boundary and its alignment with the validation data as a sanity check."
      ],
      "metadata": {
        "id": "WtE3wp3W9BDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_nn_decision_boundary(best_weights, X_valid, y_valid, forward_prop=forward_prop_deep_nn)"
      ],
      "metadata": {
        "id": "EO-w51-yOYkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW MANY HIDDEN UNITS DID THE CROSS-VALIDATION PROCEDURE CHOOSE?** (2 Points)\n",
        "\n",
        "[YOUR ANSWER HERE, EXPLAIN YOUR REASONING]\n",
        "Note: There will be randomness in your answer, likely caused by the random initialization of the weights and your specific choice of training hyperparameters.  Thus, it is perfectly reasonable for two students to have different answers to the optimal number of hidden units."
      ],
      "metadata": {
        "id": "OIC2Z3JWXp7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) Use Validation Set to Select the Number of Hidden Layers (8 Points)\n",
        "For the final part of the assignment, repeat the process we did above but to select the number of hidden layers out of the given list of candidates below.  This time, you'll need to write the cross-validation implementation yourself.  Running for up to 5 hidden layers can take 10-15 mins to complete!---so test your code with a small number of hidden layers at first to keep your development cycle fast."
      ],
      "metadata": {
        "id": "bOtfsb2vX-Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING HYPERPARAMETERS FOR THE USER TO CONFIGURE ###\n",
        "# number of hidden units in the neural network\n",
        "n_hidden_units = 3\n",
        "\n",
        "# number of hidden layers in the neural network\n",
        "n_hidden_layers_list = [1, 2, 3, 4, 5]\n",
        "\n",
        "# stepsize for gradient descent\n",
        "stepsize = 0.5\n",
        "\n",
        "# maximum number of times to iterate the gradient update\n",
        "max_iterations = 2500\n",
        "#################################################\n",
        "### HW POINTS: 6\n",
        "###################################################\n",
        "# YOUR SOLUTION GOES HERE #\n",
        "# initialize data structures to track the losses\n",
        "# Loop:\n",
        "#  initialize weights\n",
        "#  run gradient descent\n",
        "#  track losses\n",
        "#  save best parameter setting\n",
        "###################################################"
      ],
      "metadata": {
        "id": "_2io9OuOYMse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training and validation loss curves."
      ],
      "metadata": {
        "id": "8iuV57s1AYos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training and validation loss for each number of hidden layers\n",
        "plt.plot(n_hidden_layers_list, validation_losses, 'rx-', lw=3, ms=10, label=\"Validation Loss\")\n",
        "plt.plot(n_hidden_layers_list, training_losses, 'ko--', lw=3, ms=10, label=\"Training Loss\")\n",
        "plt.xlabel(\"Number of Hidden Layers\", fontsize=12)\n",
        "plt.ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Maep9BXEZ6bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW MANY HIDDEN LAYERS DID THE CROSS-VALIDATION PROCEDURE CHOOSE?** (2 Points)\n",
        "\n",
        "[YOUR ANSWER HERE, EXPLAIN YOUR REASONING]\n",
        "Note: Again, there will be randomness in your answer, likely caused by the random initialization of the weights and your specific choice of training hyperparameters.  Thus, it is perfectly reasonable for two students to have different answers to the optimal number of hidden layers."
      ],
      "metadata": {
        "id": "G-DjBfM7bkcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turning in your Solutions\n",
        "\n",
        "Turn in your solution via Gradescope (accessed through Canvas).  You need to upload *two* files:\n",
        "*   a PDF of your code and solutions as shown in the notebook.  You can generate it via File > Print > Save as PDF.  Make sure all of your code and plotting cells are visible.\n",
        "*   a Python file (file.py).  You can export one from the notebook via File > Download > Download .py.\n",
        "\n"
      ],
      "metadata": {
        "id": "v_vS0312tmR4"
      }
    }
  ]
}